# -*- coding: utf-8 -*-
"""Text_Classification_with_BERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kSLRi5-4-aUVlLSDzS_kN_823nlJZtBB
"""

!pip install datasets
!pip install tokenizers
!pip install transformers


from google.colab import drive         
drive.mount("/content/drive")        #Colab'in Google Drive'ınızdaki dosyalarınıza erişebilmesini sağlıyor.


from datasets import load_dataset
raw_datasets = load_dataset("csv", data_files = {"train": "/content/drive/MyDrive/Colab Notebooks/train_news.csv",
                                                  "test": "/content/drive/MyDrive/Colab Notebooks/test_news.csv"})


from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("dbmdz/bert-base-turkish-cased")      #BERT'in Türkçe için olan Tokenizer'ı.


def tokenize_function(examples):
    return tokenizer(examples["text"], max_length = 256, padding = "max_length", truncation = True)
tokenized_datasets = raw_datasets.map(tokenize_function, batched = True)


from torch.utils.data import DataLoader
tokenized_datasets = tokenized_datasets.remove_columns(["text"])
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
tokenized_datasets.set_format("torch")
train_dataset = tokenized_datasets["train"].shuffle(seed = 42)                  #Veri setini rastgele karıştırıyoruz
eval_dataset = tokenized_datasets["test"].shuffle(seed = 42)
train_dataloader = DataLoader(train_dataset, shuffle = True, batch_size = 8)    #Verimizi istediğimiz boyutta batch'ler haline getiriyoruz.
eval_dataloader = DataLoader(eval_dataset, batch_size = 8)




def average(batch, attention_mask):
    original_length = 0
    while original_length < len(attention_mask[0]) and attention_mask[0][original_length]:        #Attention_Mask ile ilk cümlenin gerçek boyutunu buluyoruz.
        original_length += 1                                                                      #Yani [PAD] tokenlarını saymadan kaç token olduğunu.

    sentence_embeddings = torch.mean(batch[0][1:original_length], 0, True)                        #İlk cümlenin orijinal tokenlarının ortalamasını alıyoruz.

    for i in range(1, len(batch)):                                                               
        original_length = 0
        while original_length < len(attention_mask[i]) and attention_mask[i][original_length]:
            original_length += 1
        sentence_embeddings = torch.cat((sentence_embeddings, torch.mean(batch[i][1:original_length], 0, True)), 0)   #Her cümle için üstteki 2 işlemi yapıp birleştiriyoruz.

    return sentence_embeddings




def hierarchical_avg_max(sentence, mask, window_size):
    original_length = 0
    while original_length < len(mask) and mask[original_length]:                                 
        original_length += 1

    original_window = original_length-window_size+1                                       #Cümlenin orijinal boyutu ve window_size ile kaç ayrı token grubu elde edeceğimizi hesaplıyoruz
                                                                                              
    if original_window > 2:                                                               #Token grubu sayısı 2'den fazla olacaksa işlemi olduğu gibi uyguluyoruz.
        sentence = torch.reshape(sentence, (sentence.shape[0], 1, sentence.shape[1]))     
        window = sentence[0]
        for i in range(1, window_size):
            window = torch.cat((window, sentence[i]), 0)
        windows = torch.mean(window, 0, True)                                             #İlk token grubunun ortalamasını alıyoruz.

        for i in range(1, sentence.shape[0] - window_size + 1):
            window = sentence[i]
            for j in range(i + 1, i + window_size):
                window = torch.cat((window, sentence[j]), 0)
            windows = torch.cat((windows, torch.mean(window, 0, True)), 0)               #Diğer token gruplarının ortalamasını alıp hepsini birleştiriyoruz.

        sentence_embeddings = torch.reshape(torch.max(windows[1:original_window-1], 0, True)[0], (1, sentence.shape[-1]))  #Orijinal token gruplarımız arasında maksimum alıyoruz.
    else:
        sentence_embeddings = torch.mean(sentence[1:original_length-1], 0, True)         #Token grubu sayısı <= 2 olacaksa ilk ve son token dışındaki tokenların ortalamasını alıyoruz sadece.
                                                                                         #Çünkü bu durumda ilk ve son token'ın bulunmadığı bir grup elde edemiyoruz. İlk ve son token arasındaki tokenları
    return sentence_embeddings                                                           #bir grup sayarsak, sadece bir grubumuz olduğundan gruplar arasında maksimum olanı elde etmiş oluyoruz zaten.




def hierarchical_max_avg(sentence, mask, window_size):                            #Hierarchical_avg_max ile tamamen aynı mantıkta çalışıyor.
    original_length = 0                                                           #Yalnızca ortalama ve maksimum alma sıraları farklı.
    while original_length < len(mask) and mask[original_length]:
        original_length += 1

    original_window = original_length-window_size+1

    if original_window > 2:
        sentence = torch.reshape(sentence, (sentence.shape[0], 1, sentence.shape[1]))
        window = sentence[0]
        for i in range(1, window_size):
            window = torch.cat((window, sentence[i]), 0)
        windows = torch.max(window, 0, True)[0]

        for i in range(1, sentence.shape[0] - window_size + 1):
            window = sentence[i]
            for j in range(i + 1, i + window_size):
                window = torch.cat((window, sentence[j]), 0)
            windows = torch.cat((windows, torch.max(window, 0, True)[0]), 0)

        sentence_embeddings = torch.reshape(torch.mean(windows[1:original_window-1], 0, True), (1, sentence.shape[-1]))
    else:
        sentence_embeddings = torch.max(sentence[1:original_length-1], 0, True)[0]

    return sentence_embeddings




def hierarchical(batch, attention_mask, primary = "avg", window_size = 2):                    #Genel hierarchical fonksiyonu (tüm batch için çalışır)
    if prio == "avg":
        hierarchical_prio = hierarchical_avg_max                                              #Öncelikli işlem (primary) "avg" ise çağıracağımız fonksiyon hierarchical_avg_max(bir cümle için çalışır)
    elif prio == "max":
        hierarchical_prio = hierarchical_max_avg                                              #Öncelikli işlem (primary) "max" ise çağıracağımız fonksiyon hierarchical_max_avg(bir cümle için çalışır)

    sentence_embeddings = hierarchical_prio(batch[0], attention_mask[0], window_size)         #İlk cümlenin vektörünü elde ediyoruz.
    for i in range(1, len(batch)):
        sentence_embeddings = torch.cat((sentence_embeddings, hierarchical_prio(batch[i], attention_mask[i], window_size)), 0) #Kalan cümleler için de aynı işlemi yapıp sonuçları birleştiriyoruz.

    return sentence_embeddings




import torch.nn as nn
from transformers import BertModel
import torch
class BertClassifier(nn.Module):
    def __init__(self, pretrained:str, freeze_bert = False, num_classes = 2, pooling_strategy = "cls", pooling_output_layer = -1, hierarchical_window_size = 2):
        super(BertClassifier, self).__init__()
        D_in, H, D_out = 768, 50, num_classes
        self.bert = BertModel.from_pretrained(pretrained)                                      #Kullanacağımız BERT modeli
        self.classifier = nn.Sequential(nn.Linear(D_in, H), nn.ReLU(), nn.Linear(H, D_out))    #Bizim eklediğimiz sınıflandırma katmanı
        self.pooling_strategy = pooling_strategy                                               #Tercih ettiğimiz pooling stratejisi
        self.pooling_output_layer = pooling_output_layer                                       #Pooling için çıktılarını kullanacağımız BERT katmanı
        self.hierarchical_window_size = hierarchical_window_size                               #Hierarchical Pooling stratejisi için window_size

        if freeze_bert:
            for param in self.bert.parameters():
                param.requires_grad = False

    def forward(self, input_ids, attention_mask, output_hidden_states = True):
        outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask, output_hidden_states = output_hidden_states)   #Model çıktılarını alıyoruz.
        
        if self.pooling_strategy == "cls":                                                #Tercih ettiğimiz pooling stratejisine göre pooling işlemini yapıyoruz.
            sentence_embeddings = outputs[1]
        else:
            batch = outputs[2][self.pooling_output_layer]
            if self.pooling_strategy == "average":
                sentence_embeddings = average(batch, attention_mask)
            elif self.pooling_strategy == "hierarchical_avg_max":
                sentence_embeddings = hierarchical(batch, attention_mask, "avg", self.hierarchical_window_size)
            elif self.pooling_strategy == "hierarchical_max_avg":
                sentence_embeddings = hierarchical(batch, attention_mask, "max", self.hierarchical_window_size)

        logits = self.classifier(sentence_embeddings.to(device))                         #Pooling işlemi sonuçlarını sınıflandırma katmanına veriyoruz
        return logits

    
    
    
model = BertClassifier("dbmdz/bert-base-turkish-cased", num_classes = 13, pooling_strategy = "hierarchical_avg_max", pooling_output_layer = -2, hierarchical_window_size = 3)

loss_fn = nn.CrossEntropyLoss()                     #Loss fonksiyonu


from transformers import AdamW                     
optimizer = AdamW(model.parameters(), lr = 5e-5)    #AdamW optimizer


from transformers import get_scheduler
num_epochs = 2                                                         #epoch -> Veri setini bir kere baştan sona gitmek. Tavsiye edilen epoch sayısı (2-3-4)
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler("linear", optimizer = optimizer, num_warmup_steps = 0, num_training_steps = num_training_steps)


device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")      #Daha hızlı çalışmak için cihazımızı Colab'in sağladığı GPU olarak ayarlıyoruz.
model.to(device)                                                                         #Ve modeli GPU'ya taşıyoruz.




from tqdm.auto import tqdm                                            #TRAIN VE TEST BLOKLARI
import numpy as np
import time

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_epochs):
    print(f"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}")
    print("-"*70)
    t0_epoch, t0_batch = time.time(), time.time()
    model.train()
    total_loss, batch_loss, batch_counts = 0, 0, 0

    for step, batch in enumerate(train_dataloader):
        batch_counts +=1
        b_input_ids = batch["input_ids"].to(device)
        b_attn_mask = batch["attention_mask"].to(device)
        b_labels = batch["labels"].type(torch.LongTensor).to(device)
        model.zero_grad()     
        logits = model(b_input_ids, b_attn_mask)
        loss = loss_fn(logits, b_labels)
        batch_loss += loss.item()
        total_loss += loss.item()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

        if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):
            time_elapsed = time.time() - t0_batch
            print(f"{epoch + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}")
            batch_loss, batch_counts = 0, 0
            t0_batch = time.time()

    avg_train_loss = total_loss / len(train_dataloader)
    print("-"*70)

    val_loss = []
    val_accuracy = []
    model.eval()
    for batch in eval_dataloader:
        b_input_ids = batch["input_ids"].to(device)
        b_attn_mask = batch["attention_mask"].to(device)
        b_labels = batch["labels"].type(torch.LongTensor).to(device)
        with torch.no_grad():
            logits = model(b_input_ids, b_attn_mask)

        loss = loss_fn(logits, b_labels)
        val_loss.append(loss.item())
        preds = torch.argmax(logits, dim=1).flatten()
        accuracy = (preds == b_labels).cpu().numpy().mean() * 100
        val_accuracy.append(accuracy)
        
    val_loss = np.mean(val_loss)
    val_accuracy = np.mean(val_accuracy)
    time_elapsed = time.time() - t0_epoch
    print(f"{epoch + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}")
    print("-"*70)
    print("\n")

print("Training complete!")
